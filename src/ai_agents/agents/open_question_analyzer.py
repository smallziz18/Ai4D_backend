"""
Analyseur approfondi de questions ouvertes avec GPT-4.
√âvalue le sens, la profondeur et la qualit√© des r√©ponses ouvertes.
"""
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import json

from src.config import Config


OPEN_QUESTION_ANALYSIS_PROMPT = """
Tu es un expert en √©valuation p√©dagogique sp√©cialis√© en Intelligence Artificielle.

üéØ Ta mission :
Analyser en profondeur la r√©ponse d'un apprenant √† une question ouverte sur l'IA.

‚ö†Ô∏è PRINCIPE : Les questions ouvertes r√©v√®lent le VRAI niveau de compr√©hension.

üìä CRIT√àRES D'√âVALUATION (chaque crit√®re sur 10) :

1. **Compr√©hension du concept** (0-10)
   - 0-2 : Aucune compr√©hension / R√©ponse hors-sujet
   - 3-4 : Compr√©hension tr√®s superficielle
   - 5-6 : Compr√©hension de base correcte
   - 7-8 : Bonne compr√©hension avec d√©tails
   - 9-10 : Compr√©hension approfondie et nuanc√©e

2. **Profondeur d'analyse** (0-10)
   - 0-2 : R√©ponse de surface, d√©finition Wikipedia
   - 3-4 : Quelques d√©tails mais reste superficiel
   - 5-6 : Explique le "comment" de mani√®re claire
   - 7-8 : Explique le "pourquoi" et fait des liens
   - 9-10 : Analyse critique avec implications

3. **Exemples concrets** (0-10)
   - 0-2 : Aucun exemple ou exemples incorrects
   - 3-4 : Exemples vagues ou g√©n√©riques
   - 5-6 : Exemples corrects mais basiques
   - 7-8 : Exemples pertinents et bien expliqu√©s
   - 9-10 : Exemples avanc√©s avec cas d'usage r√©els

4. **Clart√© de l'explication** (0-10)
   - 0-2 : Confus, incoh√©rent ou incompr√©hensible
   - 3-4 : Structure faible, termes mal utilis√©s
   - 5-6 : Clair mais pourrait √™tre mieux structur√©
   - 7-8 : Bien structur√© avec vocabulaire correct
   - 9-10 : Explication impeccable, p√©dagogique

üéì ESTIMATION DU NIVEAU R√âEL :
Bas√© sur le score global (moyenne des 4 crit√®res) :
- 0-2.5 : D√©butant absolu (niveau 1-2)
- 2.5-4 : D√©butant (niveau 2-3)
- 4-5.5 : Interm√©diaire bas (niveau 4-5)
- 5.5-7 : Interm√©diaire (niveau 5-6)
- 7-8 : Interm√©diaire avanc√© (niveau 7)
- 8-9 : Avanc√© (niveau 8-9)
- 9-10 : Expert (niveau 10)

üí¨ FEEDBACK CONSTRUCTIF :
- Sois pr√©cis sur ce qui est bien / ce qui manque
- Donne des pistes concr√®tes d'am√©lioration
- Encourage l'apprenant tout en √©tant honn√™te

FORMAT JSON STRICT :
{
  "scores": {
    "comprehension": 7,
    "profondeur": 6,
    "exemples": 5,
    "clarte": 7
  },
  "score_global": 6.25,
  "niveau_reel_estime": 6,
  "niveau_label": "Interm√©diaire",
  "feedback": "Bonne compr√©hension de base du concept de backpropagation. Tu expliques correctement le principe de propagation de l'erreur. Cependant, il manque des d√©tails sur la d√©rivation en cha√Æne et l'algorithme du gradient. Ajouter un exemple concret (ex: r√©seau √† 2 couches) rendrait l'explication plus claire.",
  "points_forts": [
    "Bonne utilisation du vocabulaire technique",
    "Explication claire du flow avant-arri√®re"
  ],
  "points_amelioration": [
    "D√©tailler le r√¥le de la d√©rivation en cha√Æne",
    "Ajouter un exemple num√©rique simple",
    "Expliquer la mise √† jour des poids"
  ],
  "suggestions": [
    "Regarder une vid√©o anim√©e sur la backpropagation pour visualiser le processus",
    "Impl√©menter un petit r√©seau from scratch pour comprendre les calculs",
    "Lire sur l'historique : algorithme de Rumelhart, Hinton et Williams (1986)"
  ]
}
"""


class OpenQuestionAnalyzer:
    """Analyseur approfondi de questions ouvertes avec GPT-4."""

    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o",  # GPT-4 pour analyse de qualit√©
            api_key=Config.OPENAI_API_KEY,
            temperature=0.3  # Peu de cr√©ativit√©, plus de pr√©cision
        )

    async def analyze_open_question(
        self,
        question: str,
        user_answer: str,
        expected_answer: str = None
    ) -> Dict[str, Any]:
        """
        Analyse une r√©ponse √† une question ouverte en profondeur.

        Args:
            question: La question pos√©e
            user_answer: La r√©ponse de l'utilisateur
            expected_answer: R√©ponse attendue/correction (optionnel)

        Returns:
            Dict avec scores d√©taill√©s et feedback
        """
        # Cas de r√©ponse vide
        if not user_answer or not str(user_answer).strip():
            return {
                "scores": {
                    "comprehension": 0,
                    "profondeur": 0,
                    "exemples": 0,
                    "clarte": 0
                },
                "score_global": 0.0,
                "niveau_reel_estime": 1,
                "niveau_label": "D√©butant absolu",
                "feedback": "Aucune r√©ponse fournie. Il est essentiel de r√©pondre aux questions ouvertes pour √©valuer ton niveau r√©el.",
                "points_forts": [],
                "points_amelioration": ["Prendre le temps de formuler une r√©ponse"],
                "suggestions": ["R√©essayer en expliquant avec tes propres mots"]
            }

        context = ""
        if expected_answer:
            context = f"\n\nR√âPONSE ATTENDUE (pour r√©f√©rence) :\n{expected_answer}"

        messages = [
            SystemMessage(content=OPEN_QUESTION_ANALYSIS_PROMPT),
            HumanMessage(content=f"""
Analyse cette r√©ponse √† une question ouverte d'√©valuation en IA.

QUESTION :
{question}

R√âPONSE DE L'UTILISATEUR :
{user_answer}{context}

√âvalue sur les 4 crit√®res (0-10 chacun) :
1. Compr√©hension du concept
2. Profondeur d'analyse
3. Exemples concrets
4. Clart√© de l'explication

Retourne un JSON avec les scores, le niveau estim√© et un feedback constructif.
            """)
        ]

        try:
            response = await self.llm.ainvoke(messages)
            response_text = response.content

            # Nettoyer et parser JSON
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()

            analysis = json.loads(response_text)

            # Validation basique
            if not isinstance(analysis, dict) or "scores" not in analysis:
                raise ValueError("Format d'analyse invalide")

            return analysis

        except Exception as e:
            print(f"‚ùå Erreur analyse question ouverte: {e}")
            # Fallback en cas d'erreur
            return {
                "scores": {
                    "comprehension": 5,
                    "profondeur": 5,
                    "exemples": 5,
                    "clarte": 5
                },
                "score_global": 5.0,
                "niveau_reel_estime": 5,
                "niveau_label": "Interm√©diaire",
                "feedback": "Analyse automatique indisponible. R√©ponse enregistr√©e.",
                "points_forts": [],
                "points_amelioration": [],
                "suggestions": [],
                "error": str(e)
            }

    async def analyze_multiple_open_questions(
        self,
        questions_and_answers: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """
        Analyse plusieurs questions ouvertes en batch.

        Args:
            questions_and_answers: Liste de {question, user_answer, expected_answer}

        Returns:
            Dict avec analyses individuelles et synth√®se globale
        """
        analyses = []

        for qa in questions_and_answers:
            analysis = await self.analyze_open_question(
                question=qa.get("question", ""),
                user_answer=qa.get("user_answer", ""),
                expected_answer=qa.get("expected_answer")
            )

            # Ajouter la question au r√©sultat
            analysis["question"] = qa.get("question", "")
            analysis["user_answer"] = qa.get("user_answer", "")

            analyses.append(analysis)

        # Calculer la moyenne globale
        if analyses:
            avg_score = sum(a["score_global"] for a in analyses) / len(analyses)

            # Estimer le niveau global
            if avg_score <= 2.5:
                niveau_global = 2
                niveau_label_global = "D√©butant"
            elif avg_score <= 4:
                niveau_global = 3
                niveau_label_global = "D√©butant+"
            elif avg_score <= 5.5:
                niveau_global = 5
                niveau_label_global = "Interm√©diaire bas"
            elif avg_score <= 7:
                niveau_global = 6
                niveau_label_global = "Interm√©diaire"
            elif avg_score <= 8:
                niveau_global = 7
                niveau_label_global = "Interm√©diaire avanc√©"
            elif avg_score <= 9:
                niveau_global = 8
                niveau_label_global = "Avanc√©"
            else:
                niveau_global = 10
                niveau_label_global = "Expert"
        else:
            avg_score = 0
            niveau_global = 1
            niveau_label_global = "Non √©valu√©"

        # Agr√©ger les points forts/am√©lioration
        all_strengths = []
        all_improvements = []
        all_suggestions = []

        for a in analyses:
            all_strengths.extend(a.get("points_forts", []))
            all_improvements.extend(a.get("points_amelioration", []))
            all_suggestions.extend(a.get("suggestions", []))

        # D√©dupliquer
        all_strengths = list(set(all_strengths))
        all_improvements = list(set(all_improvements))
        all_suggestions = list(set(all_suggestions))

        return {
            "analyses_individuelles": analyses,
            "synthese_globale": {
                "score_moyen": round(avg_score, 2),
                "niveau_estime": niveau_global,
                "niveau_label": niveau_label_global,
                "nombre_questions": len(analyses),
                "points_forts_globaux": all_strengths[:5],  # Top 5
                "points_amelioration_globaux": all_improvements[:5],
                "suggestions_globales": all_suggestions[:5]
            }
        }


# Instance globale
open_question_analyzer = OpenQuestionAnalyzer()

